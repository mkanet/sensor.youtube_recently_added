import logging
import os
import io
import json
import asyncio
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
from homeassistant.components.sensor import SensorEntity
from homeassistant.helpers.storage import Store
from homeassistant.helpers.update_coordinator import CoordinatorEntity, DataUpdateCoordinator, UpdateFailed
from homeassistant.core import HomeAssistant, callback
from homeassistant.components import persistent_notification
from .const import DOMAIN, CONF_MAX_REGULAR_VIDEOS, CONF_MAX_SHORT_VIDEOS, DEFAULT_MAX_REGULAR_VIDEOS, DEFAULT_MAX_SHORT_VIDEOS, CONF_FAVORITE_CHANNELS, DEFAULT_FAVORITE_CHANNELS, CONF_USE_COMMENTS, get_USE_COMMENTS_AS_SUMMARY
from dateutil import parser
import async_timeout
from homeassistant.helpers.aiohttp_client import async_get_clientsession
from .youtube_api import YouTubeAPI, QuotaExceededException
import numpy as np
from PIL import Image, ImageDraw
import aiofiles
import isodate
from isodate import parse_duration
import re

_LOGGER = logging.getLogger(__name__)

def remove_black_bars(img, dark_threshold=35, padding_ratio_threshold=(0.35, 0.8)):
    """
    Remove black bars using the same detection logic as check_portrait.
    """
    width, height = img.size
    target_ratio = 9 / 16  # Expected aspect ratio for Shorts
    content_ratio = width / height

    if content_ratio > target_ratio:
        expected_width = height * target_ratio
        padding_total = width - expected_width
        padding_ratio = padding_total / width

        img_gray = img.convert('L')
        left_side = np.array(img_gray.crop((0, 0, width*0.4, height)))
        right_side = np.array(img_gray.crop((width*0.6, 0, width, height)))
        dark_pixels_left = np.sum(left_side < dark_threshold) / left_side.size
        dark_pixels_right = np.sum(right_side < dark_threshold) / right_side.size

        if padding_ratio_threshold[0] <= padding_ratio <= padding_ratio_threshold[1] and dark_pixels_left > 0.75 and dark_pixels_right > 0.75:
            left = int(padding_total / 2)
            right = width - left
            cropped_img = img.crop((left, 0, right, height))
            return cropped_img

    # If no significant padding, return the original image
    return img

def replace_white_border_with_black(img, threshold=250):
    """
    Replace any white or near-white strip at the bottom of the image with black.
    """
    # Convert image to RGB if not already
    if img.mode != 'RGB':
        img = img.convert('RGB')
    
    img_np = np.array(img)
    
    # Check the bottom rows of pixels to see if they are white (or near-white)
    bottom_row = img_np[-1, :, :]  # The last row (bottom strip)
    if np.all(bottom_row > threshold):  # If all pixels in the bottom row are near white
        # Replace the bottom strip with black
        draw = ImageDraw.Draw(img)
        width, height = img.size
        # Draw a black rectangle at the bottom
        draw.rectangle([(0, height - 10), (width, height)], fill=(0, 0, 0))  # Adjust 10px as needed
    
    return img

async def process_image_to_portrait(hass: HomeAssistant, url: str, video_id) -> str:
    """
    Process the image to create a portrait thumbnail with a 2:3 aspect ratio.
    """
    save_dir = hass.config.path("www", "youtube_thumbnails")
    os.makedirs(save_dir, exist_ok=True)

    if isinstance(video_id, dict):
        if video_id.get('is_live'):
            file_name = f"{video_id.get('videoId', '')}_live_portrait.jpg"
        else:
            file_name = f"{video_id.get('videoId', '')}_portrait.jpg"
    else:
        file_name = f"{video_id}_portrait.jpg"
    file_path = os.path.join(save_dir, file_name)

    if not url:
        _LOGGER.error(f"No URL provided for video_id: {video_id}")
        return ""

    session = async_get_clientsession(hass)
    try:
        async with session.get(url) as resp:
            if resp.status == 404:
                _LOGGER.debug(f"Thumbnail not yet available for video {video_id} (404 response)")
                return url
            elif resp.status != 200:
                _LOGGER.error(f"Failed to download image from URL: {url} with status code: {resp.status}")
                return url
            image_data = await resp.read()
    except Exception as e:
        _LOGGER.error(f"Exception occurred while fetching image from URL: {url}, error: {e}")
        return ""

    try:
        _LOGGER.debug(f"Processing thumbnail for video_id {video_id} from URL: {url}")
        with Image.open(io.BytesIO(image_data)) as img:
            _LOGGER.debug(f"Original image size for video_id {video_id}: {img.size}")

            if isinstance(video_id, dict) and video_id.get('is_live'):
                img_resized = img.resize((1280, 720), Image.LANCZOS)
                image_byte_array = io.BytesIO()
                img_resized.save(image_byte_array, format="JPEG", quality=100)
                async with aiofiles.open(file_path, "wb") as f:
                    await f.write(image_byte_array.getvalue())
                return f"/local/youtube_thumbnails/{file_name}"

            img_no_black_bars = remove_black_bars(img, dark_threshold=35, padding_ratio_threshold=(0.35, 0.8))

            # Extract the center 9:16 content using same logic as check_portrait
            width, height = img_no_black_bars.size
            target_ratio = 9 / 16
            content_ratio = width / height

            _LOGGER.debug(f"Initial crop - width: {width}, height: {height}, ratio: {content_ratio:.4f}")

			# Calculate dimensions for extracting the center content
            expected_width = height * target_ratio
            padding_total = width - expected_width
            if padding_total > 0 and 0.35 <= padding_total / width <= 0.8 and img_no_black_bars != img:
                # Calculate potential crop dimensions
                left = int(padding_total / 2)
                right = width - int(padding_total / 2)
                potential_width = right - left
                
                # Add check for 9:16 ratio of center content with dynamic tolerance
                center_ratio = potential_width / height
                ratio_tolerance = 0.1  # Default for maxresdefault
                if any(x in url for x in ['hqdefault', 'sddefault', 'mqdefault', 'default']):
                    ratio_tolerance = 0.2
                if abs(center_ratio - (9/16)) <= ratio_tolerance:  
                    cropped_img = img_no_black_bars.crop((left, 0, right, height))
                    _LOGGER.debug(f"Extracted center content: {cropped_img.size}")
                else:
                    # Fall back to regular image handling
                    if width >= height * target_ratio:
                        target_width = height * target_ratio
                        left = (width - target_width) // 2
                        right = left + target_width
                        cropped_img = img_no_black_bars.crop((left, 0, right, height))
                _LOGGER.debug(f"Extracted center content: {cropped_img.size}")
            else:
                # Regular image, crop to 9:16
                if width >= height * target_ratio:
                    target_width = height * target_ratio
                    left = (width - target_width) // 2
                    right = left + target_width
                    cropped_img = img_no_black_bars.crop((left, 0, right, height))
                    _LOGGER.debug(f"Extracted center content: {cropped_img.size}")
                else:
                    target_height = width / target_ratio
                    top = (height - target_height) // 2
                    bottom = top + target_height
                    cropped_img = img_no_black_bars.crop((0, top, width, bottom))

            # Now crop to 2:3 for Upcoming Media Card
            width, height = cropped_img.size
            target_ratio = 2 / 3
            _LOGGER.debug(f"Adjusting to 2:3 ratio - Current size: {width}x{height}")

            if width / height > target_ratio:
                new_width = int(height * target_ratio)
                left = (width - new_width) // 2
                right = left + new_width
                img_cropped = cropped_img.crop((left, 0, right, height))
            else:
                new_height = int(width / target_ratio)
                top = (height - new_height) // 2
                bottom = top + new_height
                img_cropped = cropped_img.crop((0, top, width, bottom))

            _LOGGER.debug(f"Pre-resize size: {img_cropped.size}")
            img_resized = img_cropped.resize((480, 720), Image.LANCZOS)
            _LOGGER.debug(f"Final size: 480x720")

            image_byte_array = io.BytesIO()
            img_resized.save(image_byte_array, format="JPEG", quality=100)
            async with aiofiles.open(file_path, "wb") as f:
                await f.write(image_byte_array.getvalue())

    except Exception as e:
        _LOGGER.error(f"Error processing image for video_id {video_id}: {e}", exc_info=True)
        return "/local/youtube_thumbnails/default_thumbnail.jpg"

    return f"/local/youtube_thumbnails/{file_name}"

async def process_image_to_fanart(hass: HomeAssistant, video_id, thumbnails: dict) -> str:
    save_dir = hass.config.path("www", "youtube_thumbnails")
    os.makedirs(save_dir, exist_ok=True)

    if isinstance(video_id, dict):
        if video_id.get('is_live'):
            file_name = f"{video_id.get('videoId', '')}_live_fanart.jpg"
        else:
            file_name = f"{video_id.get('videoId', '')}_fanart.jpg"
    else:
        file_name = f"{video_id}_fanart.jpg"
    file_path = os.path.join(save_dir, file_name)  # Added this line

    url = thumbnails.get('maxres', {}).get('url')
    if not url:
        url = thumbnails.get('standard', {}).get('url') or thumbnails.get('high', {}).get('url')

    if not url:
        _LOGGER.error(f"No suitable thumbnail found for video_id: {video_id}")
        return ""

    session = async_get_clientsession(hass)
    try:
        async with session.get(url) as resp:
            if resp.status != 200:
                _LOGGER.error(f"Failed to download image from URL: {url} with status code: {resp.status}")
                return url
            image_data = await resp.read()
    except Exception as e:
        _LOGGER.error(f"Exception occurred while fetching image from URL: {url}, error: {e}")
        return ""

    try:
        with Image.open(io.BytesIO(image_data)) as img:
            _LOGGER.debug(f"Processing image for video_id {video_id} - Original size: {img.size}")
            img_no_black_bars = img.convert("RGB")
            width, height = img_no_black_bars.size
            pixels = img_no_black_bars.load()

            # Detect black bars dynamically
            top, bottom, left, right = 0, height - 1, 0, width - 1
            threshold = 30

            # Detect top black bar
            for y in range(height):
                if any(not all(v <= threshold for v in pixels[x, y]) for x in range(width)):
                    top = y
                    break

            # Detect bottom black bar
            for y in range(height - 1, -1, -1):
                if any(not all(v <= threshold for v in pixels[x, y]) for x in range(width)):
                    bottom = y
                    break

            # Detect left black bar
            for x in range(width):
                if any(not all(v <= threshold for v in pixels[x, y]) for y in range(height)):
                    left = x
                    break

            # Detect right black bar
            for x in range(width - 1, -1, -1):
                if any(not all(v <= threshold for v in pixels[x, y]) for y in range(height)):
                    right = x
                    break

            # Log black bar detection results
            if left > 0 or top > 0 or right < width - 1 or bottom < height - 1:
                _LOGGER.debug(
                    f"Black bars detected - Left: {left}px, Top: {top}px, "
                    f"Right: {width - right - 1}px, Bottom: {height - bottom - 1}px"
                )

            # Crop to remove black bars
            img_cropped = img_no_black_bars.crop((left, top, right + 1, bottom + 1))
            width, height = img_cropped.size
            _LOGGER.debug(f"Size after black bar removal: {width}x{height}")

            # Adjust to 16:9 if necessary
            target_ratio = 16 / 9
            current_ratio = width / height
            if abs(current_ratio - target_ratio) > 0.01:
                _LOGGER.debug(f"Adjusting aspect ratio from {current_ratio:.2f} to {target_ratio}")
                if current_ratio > target_ratio:
                    new_height = int(width / target_ratio)
                    top = (height - new_height) // 2
                    _LOGGER.debug(f"Cropping {height - new_height}px from height to achieve 16:9")
                    img_cropped = img_cropped.crop((0, top, width, top + new_height))
                else:
                    new_width = int(height * target_ratio)
                    left = (width - new_width) // 2
                    _LOGGER.debug(f"Cropping {width - new_width}px from width to achieve 16:9")
                    img_cropped = img_cropped.crop((left, 0, left + new_width, height))

            # Resize to 1280x720
            img_resized = img_cropped.resize((1280, 720), Image.LANCZOS)
            _LOGGER.debug("Final resize to 1280x720 completed")
            img_byte_arr = io.BytesIO()
            img_resized.save(img_byte_arr, format='JPEG', quality=100)
            img_byte_arr = img_byte_arr.getvalue()

            async with aiofiles.open(file_path, 'wb') as out_file:
                await out_file.write(img_byte_arr)

    except Exception as e:
        _LOGGER.error(f"Error processing image for video_id {video_id}: {e}", exc_info=True)
        if isinstance(video_id, dict) and video_id.get('is_live'):
            return f"/local/youtube_thumbnails/{video_id.get('videoId', '')}_live_fanart.jpg"
        return "/local/youtube_thumbnails/default_thumbnail.jpg"

    return f"/local/youtube_thumbnails/{file_name}"

async def async_setup_entry(hass: HomeAssistant, entry, async_add_entities):
    """Set up YouTube Recently Added sensor platform."""
    # Use the existing coordinator from hass.data
    coordinator = hass.data[DOMAIN][entry.entry_id + "_coordinator"]

    sensor = YouTubeRecentlyAddedSensor(coordinator)
    shorts_sensor = YouTubeRecentlyAddedShortsSensor(coordinator)
    favorites_sensor = YouTubeRecentlyAddedFavoriteChannelsSensor(coordinator)

    async_add_entities([sensor, shorts_sensor, favorites_sensor], True)
    _LOGGER.debug("YouTube Recently Added sensors set up completed.")

class YouTubeDataUpdateCoordinator(DataUpdateCoordinator):
    def __init__(self, hass: HomeAssistant, youtube, entry):
        super().__init__(
            hass,
            _LOGGER,
            name="YouTube Recently Added",
            update_interval=timedelta(hours=6),
        )
        self.youtube = youtube
        self.hass = hass
        self.data = {"data": [], "shorts_data": [], "favorite_channels": []}
        self.config_entry = entry
        self.store = Store(hass, 1, f"{DOMAIN}.{entry.entry_id}")
        self.last_webhook_time = None
        self.favorite_channels = [channel.strip() for channel in entry.options.get(CONF_FAVORITE_CHANNELS, DEFAULT_FAVORITE_CHANNELS).split(',') if channel.strip()]
        asyncio.create_task(self._schedule_favorites_reset())

    async def _schedule_favorites_reset(self):
        async def reset_favorites(initial_delay):
            delay = initial_delay
            while True:
                await asyncio.sleep(delay)
                self.data["favorite_channels"] = []
                await self.store.async_save(self.data)
                self.async_set_updated_data(self.data)
                now = datetime.now(ZoneInfo(self.hass.config.time_zone))
                midnight = (now + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
                delay = (midnight - now).total_seconds()
        
        now = datetime.now(ZoneInfo(self.hass.config.time_zone))
        midnight = (now + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
        initial_delay = (midnight - now).total_seconds()
        
        asyncio.create_task(reset_favorites(initial_delay))

    async def async_setup(self):
        stored_data = await self.store.async_load()
        _LOGGER.critical(f"Loaded stored_data: {stored_data}")  # Add this line
        if stored_data:
            self.data = {
                "data": stored_data.get("data", []),
                "shorts_data": stored_data.get("shorts_data", []),
                "favorite_channels": stored_data.get("favorite_channels", [])
            }
            _LOGGER.critical(f"Initialized self.data: {self.data}")  # Add this line
            self.last_webhook_time = stored_data.get("last_webhook_time")
            if self.last_webhook_time:
                self.last_webhook_time = datetime.fromisoformat(self.last_webhook_time)

    async def handle_webhook_update(self, video_id, is_deleted):
        # Log the initiation of the webhook update handling with video ID and deletion status
        _LOGGER.critical(f"handle_webhook_update called for video {video_id}, is_deleted: {is_deleted}")
        
        # Log the current setting for using comments as summary
        _LOGGER.critical(f"USE_COMMENTS_AS_SUMMARY setting: {get_USE_COMMENTS_AS_SUMMARY()}")
        
        if is_deleted:
            # Log that the video is marked for deletion
            _LOGGER.critical(f"Processing deletion for video {video_id}")
            
            # Remove the video from the main data and shorts data lists
            self.data["data"] = [v for v in self.data.get("data", []) if v["id"] != video_id]
            self.data["shorts_data"] = [v for v in self.data.get("shorts_data", []) if v["id"] != video_id]
            
            # Save the updated data asynchronously
            _LOGGER.debug(f"Saving updated data after deletion of video {video_id}")
            await self.store.async_save(self.data)
        else:
            # Log that the system is fetching video details for the given video ID
            _LOGGER.critical(f"Fetching video details for {video_id}")
            
            # Fetch video data asynchronously using the YouTube API
            video_data = await self.youtube.get_video_by_id(video_id)
            
            if not video_data:
                # Log that no video data was returned and the video will be skipped
                _LOGGER.critical(f"Skipping video {video_id} because get_video_by_id returned None.")
                return
            
            if video_data:
                # Log successful retrieval of video data
                _LOGGER.critical(f"Successfully fetched video data for {video_id}")
                
                # Extract relevant details from the video data
                duration = video_data['contentDetails'].get('duration', 'PT0M0S')
                live_streaming_details = video_data.get('liveStreamingDetails', {})
                live_broadcast_content = video_data['snippet'].get('liveBroadcastContent')
                actual_start_time = live_streaming_details.get('actualStartTime')
                actual_end = live_streaming_details.get('actualEndTime')
                scheduled_start = live_streaming_details.get('scheduledStartTime')
                
                # Determine if the video is a live stream based on various criteria
                is_live_stream = (
                    live_broadcast_content in ['live', 'upcoming'] or
                    duration in ['P0D', 'PT0M0S'] or
                    actual_start_time is not None or
                    scheduled_start is not None
                ) and not actual_end
                
                # Log the live stream status of the video
                _LOGGER.critical(f"Video {video_id} live stream status: {is_live_stream}")
                
                if is_live_stream:
                    if scheduled_start and not actual_start_time:
                        # Parse the scheduled start time and convert it to UTC
                        scheduled_time = datetime.strptime(scheduled_start, '%Y-%m-%dT%H:%M:%SZ').replace(tzinfo=ZoneInfo("UTC"))
                        
                        # Log the scheduled time and check if the stream is in the future
                        _LOGGER.debug(f"Scheduled start time for video {video_id}: {scheduled_time}")
                        
                        if scheduled_time > datetime.now(ZoneInfo("UTC")):
                            # Log that the future scheduled stream is being skipped
                            _LOGGER.critical(f"Skipping future scheduled stream {video_id} - Not yet started")
                            return
                        
                        if live_broadcast_content == 'upcoming' and scheduled_time <= datetime.now(ZoneInfo("UTC")):
                            _LOGGER.debug(f"Processing started premiere for video {video_id}")
                            pass
                        else:
                            # Log that the offline scheduled stream is being skipped due to missing actual start time
                            _LOGGER.critical(f"Skipping offline scheduled stream {video_id} - Start time passed but actual_start_time missing")
                            return
                    
                    if actual_start_time and not actual_end:
                        # Retrieve and format the number of concurrent viewers
                        concurrent_viewers = int(live_streaming_details.get('concurrentViewers', '0'))
                        formatted_viewers = str(concurrent_viewers) if concurrent_viewers < 1000 else (
                            f"{concurrent_viewers//1000}K" if concurrent_viewers < 1000000 else f"{concurrent_viewers//1000000}M"
                        )
                        
                        # Set the live status with the number of viewers
                        live_status = f"🔴 LIVE - {formatted_viewers} Watching"
                        video_data['live_status'] = live_status
                        _LOGGER.debug(f"Set live_status for video {video_id}: {live_status}")
                    
                    # Reset runtime for live streams
                    video_data['runtime'] = ""
                    
                    # Initialize retry parameters for fetching viewers
                    max_retries = 5
                    retry_delay = 15
                    live_status = ""
                    
                    if actual_start_time and not actual_end:
                        # Loop to attempt fetching viewers up to max_retries
                        for attempt in range(max_retries):
                            _LOGGER.critical(f"Attempting to fetch viewers (Attempt {attempt + 1}/{max_retries}) for video {video_id}")
                            
                            # Batch update video statistics and comments
                            stats_and_comments = await self.youtube.batch_update_video_statistics_and_comments([video_id])
                            
                            if stats_and_comments and video_id in stats_and_comments:
                                # Update video statistics with fetched data
                                if 'statistics' not in video_data:
                                    video_data['statistics'] = {}
                                video_data['statistics'].update(stats_and_comments[video_id])
                                
                                # Update summary if comments are available
                                if 'comments' in stats_and_comments[video_id]:
                                    video_data['summary'] = stats_and_comments[video_id]['comments']
                                
                                # Retrieve and format concurrent viewers
                                concurrent_viewers = int(stats_and_comments[video_id].get('concurrentViewers', '0'))
                                value = concurrent_viewers / 1000000 if concurrent_viewers >= 1000000 else concurrent_viewers / 1000 if concurrent_viewers >= 1000 else concurrent_viewers
                                suffix = 'M' if concurrent_viewers >= 1000000 else 'K' if concurrent_viewers >= 1000 else ''
                                formatted_viewers = (f"{value:.1f}".rstrip('0').rstrip('.') + suffix) if concurrent_viewers >= 1000 else str(concurrent_viewers)
                                
                                # Determine live broadcast content type and set live status accordingly
                                live_broadcast_content = video_data['snippet'].get('liveBroadcastContent')
                                if live_broadcast_content == 'upcoming':
                                    if concurrent_viewers > 0:
                                        live_status = f"🔴 PREMIERE - {formatted_viewers} Watching"
                                    else:
                                        live_status = "🔴 PREMIERE"
                                else:
                                    if concurrent_viewers > 0:
                                        live_status = f"🔴 LIVE - {formatted_viewers} Watching"
                                    else:
                                        live_status = "🔴 LIVE"
                                
                                # Log the detection of a live stream with current viewer count
                                _LOGGER.critical(f"Live stream detected for video {video_id} - Currently streaming with {concurrent_viewers} viewers")
                                
                                # If viewers are present or it's the last retry, update the video data and save
                                if concurrent_viewers > 0 or attempt == max_retries - 1:
                                    video_data['live_status'] = live_status
                                    video_data['airdate'] = datetime.now(ZoneInfo("UTC")).isoformat()
                                    target_list = 'data'
                                    
                                    # Ensure the target list exists in data
                                    if target_list not in self.data:
                                        self.data[target_list] = []
                                    
                                    # Remove existing entry and insert the updated video data at the top
                                    self.data[target_list] = [v for v in self.data[target_list] if v['id'] != video_data['id']]
                                    self.data[target_list].insert(0, video_data)
                                    
                                    # Save the updated data and set it as the current data
                                    _LOGGER.debug(f"Saving updated data after processing live stream for video {video_id}")
                                    await self.store.async_save(self.data)
                                    self.async_set_updated_data(self.data)
                                    break
                                else:
                                    # Log that there are zero viewers and a retry will be attempted after delay
                                    _LOGGER.critical(f"Zero viewers, waiting {retry_delay}s before retry for video {video_id}")
                                    await asyncio.sleep(retry_delay)
                            else:
                                # Log a warning if no statistics are returned and a retry will be attempted
                                _LOGGER.warning(f"No statistics returned for video {video_id}. Retrying if applicable.")
                                await asyncio.sleep(retry_delay)
                        
                        # Final fallback after all retries if live_status was not set
                        if 'live_status' not in video_data:
                            live_status = "🔴 LIVE - 0 Watching"
                            video_data['live_status'] = live_status
                            video_data['airdate'] = datetime.now(ZoneInfo("UTC")).isoformat()
                            target_list = 'data'
                            
                            if target_list not in self.data:
                                self.data[target_list] = []
                            
                            self.data[target_list] = [v for v in self.data[target_list] if v['id'] != video_data['id']]
                            self.data[target_list].insert(0, video_data)
                            
                            _LOGGER.debug(f"Final fallback: Saving live_status for video {video_id} with 0 viewers")
                            await self.store.async_save(self.data)
                            self.async_set_updated_data(self.data)
                    
                    elif actual_end:
                        # Fetch updated statistics and comments if the live stream has ended
                        stats_and_comments = await self.youtube.batch_update_video_statistics_and_comments([video_id])
                        if stats_and_comments and video_id in stats_and_comments:
                            if 'statistics' not in video_data:
                                video_data['statistics'] = {}
                            video_data['statistics'].update(stats_and_comments[video_id])
                            
                            if 'comments' in stats_and_comments[video_id]:
                                video_data['summary'] = stats_and_comments[video_id]['comments']
                            
                            # Set live status to indicate the stream has ended
                            video_data['live_status'] = "📴 Stream ended"
                            video_data['airdate'] = datetime.now(ZoneInfo("UTC")).isoformat()
                            target_list = 'data'
                            
                            if target_list not in self.data:
                                self.data[target_list] = []
                            
                            self.data[target_list] = [v for v in self.data[target_list] if v['id'] != video_data['id']]
                            self.data[target_list].insert(0, video_data)
                            
                            # Save the updated data and set it as the current data
                            _LOGGER.critical(f"Live stream detected for video {video_id} - Stream ended at {actual_end}")
                            _LOGGER.debug(f"Saving updated data after stream end for video {video_id}")
                            await self.store.async_save(self.data)
                            self.async_set_updated_data(self.data)
                    
                    # Reset runtime after handling live stream details
                    video_data['runtime'] = ""
                    
                    # Process thumbnails to generate fanart and poster images
                    thumbnails = video_data['snippet'].get('thumbnails', {})
                    video_data['fanart'] = await process_image_to_fanart(self.hass, video_id, thumbnails)
                    
                    if is_live_stream and not actual_end:
                        # Set the poster URL for live streams
                        poster_url = f"https://i.ytimg.com/vi/{video_id}/maxresdefault_live.jpg"
                    else:
                        # Set the poster URL for regular videos
                        poster_url = thumbnails.get('maxres', {}).get('url') or thumbnails.get('high', {}).get('url')
                    
                    if poster_url:
                        # Prepare data for processing the portrait image
                        video_id_data = {'videoId': video_id, 'is_live': bool(is_live_stream and not actual_end)}
                        video_data['poster'] = await process_image_to_portrait(self.hass, poster_url, video_id_data)
                    else:
                        # Set a default poster if no poster URL is available
                        video_data['poster'] = "/local/youtube_thumbnails/default_poster.jpg" if not is_live_stream else f"/local/youtube_thumbnails/{video_id}_live_portrait.jpg"
                    
                    # Assign the live status to the video data
                    video_data['live_status'] = live_status
                    
                    target_list = 'data'
                    max_videos = self.config_entry.options.get(CONF_MAX_REGULAR_VIDEOS, DEFAULT_MAX_REGULAR_VIDEOS)
                    
                    if target_list not in self.data:
                        self.data[target_list] = []
                    
                    # Remove existing entries of the video and insert the updated video data
                    self.data[target_list] = [v for v in self.data.get(target_list, []) if v['id'] != video_data['id']]
                    self.data['shorts_data'] = [v for v in self.data.get('shorts_data', []) if v['id'] != video_data['id']]
                    
                    # Set the airdate based on whether it's a live stream or a regular video
                    video_data['airdate'] = datetime.now(ZoneInfo("UTC")).isoformat() if is_live_stream else video_data.get('snippet', {}).get('publishedAt')
                    self.data[target_list].insert(0, video_data)
                    
                    # Sort the videos by airdate and limit the number of videos to max_videos
                    self.data[target_list] = sorted(self.data[target_list], key=lambda x: x.get('airdate', ''), reverse=True)[:max_videos]
                    
                    # Save the updated data asynchronously
                    _LOGGER.debug(f"Saving updated data after processing video {video_id} to {target_list}")
                    await self.store.async_save(self.data)
                    
                    # Check if the video belongs to any favorite channels and update accordingly
                    if video_data and any(channel.lower() in video_data.get("snippet", {}).get("channelTitle", "").lower() 
                                        for channel in self.favorite_channels if channel):
                        _LOGGER.debug(f"Video {video_id} is from a favorite channel. Updating favorite_channels list.")
                        self.data["favorite_channels"] = [v for v in self.data.get("favorite_channels", []) if v["id"] != video_data["id"]]
                        self.data["favorite_channels"].insert(0, video_data)
                    
                    # Set the updated data
                    self.async_set_updated_data(self.data)
                    
                    # Log that the live stream video has been processed and saved
                    _LOGGER.critical(f"Live stream video {video_id} processed and saved")
                    return
                
                try:
                    # Parse the duration of the video and convert it to seconds
                    duration_timedelta = parse_duration(duration)
                    duration_seconds = int(duration_timedelta.total_seconds())
                    _LOGGER.critical(f"Video {video_id} duration: {duration_seconds} seconds")
                except (ValueError, TypeError):
                    # Handle invalid duration formats
                    duration_seconds = 0
                    _LOGGER.critical(f"Invalid duration format for video {video_id}: {duration}")
                
                # Fetch statistics and comments for the video
                stats_and_comments = await self.youtube.batch_update_video_statistics_and_comments([video_id])
                if stats_and_comments and video_id in stats_and_comments:
                    if 'statistics' not in video_data:
                        video_data['statistics'] = {}
                    video_data['statistics'].update(stats_and_comments[video_id])
                    
                    if 'live_status' in stats_and_comments[video_id]:
                        video_data['live_status'] = stats_and_comments[video_id]['live_status']
                    
                    if 'comments' in stats_and_comments[video_id]:
                        video_data['summary'] = stats_and_comments[video_id]['comments']
                
                is_short = False
                if duration_seconds <= 60:
                    # Classify the video as a short if its duration is 60 seconds or less
                    _LOGGER.critical(f"Video {video_id} classified as short (duration <= 60s)")
                    is_short = True
                elif duration_seconds <= 180:
                    async def check_portrait(url):
                        if not url:
                            return False
                        try:
                            # Fetch the image from the URL asynchronously
                            session = async_get_clientsession(self.hass)
                            async with session.get(url) as resp:
                                if resp.status != 200:
                                    _LOGGER.debug(f"Failed to fetch image from URL: {url} with status {resp.status}")
                                    return False
                                image_data = await resp.read()
                            
                            # Open the image using PIL
                            with Image.open(io.BytesIO(image_data)) as img:
                                width, height = img.size
                                _LOGGER.debug(f"Image dimensions for portrait check: {width}x{height}")
                                
                                # Skip processing if the image is square (likely a profile picture)
                                if width == height:
                                    _LOGGER.debug("Image is square; skipping portrait check.")
                                    return False
                                
                                # If the image is already in portrait format, return True
                                if width < height:
                                    _LOGGER.debug("Image is in portrait orientation.")
                                    return True
                                
                                img_gray = img.convert('L')
                                
                                # Calculate the width of each third of the image
                                third_width = width // 3
                                
                                # Crop the image into three vertical sections
                                left_side = np.array(img_gray.crop((0, 0, third_width, height)))
                                center = np.array(img_gray.crop((third_width, 0, third_width * 2, height)))
                                right_side = np.array(img_gray.crop((third_width * 2, 0, width, height)))
                                
                                # Calculate the mean brightness of each section
                                left_mean = np.mean(left_side)
                                right_mean = np.mean(right_side)
                                center_mean = np.mean(center)
                                
                                # Calculate the 20th percentile brightness for the left and right sides
                                left_dark = np.percentile(left_side, 20)
                                right_dark = np.percentile(right_side, 20)
                                
                                # Determine if the edges are significantly darker than the center
                                edges_darker = (
                                    left_mean < center_mean * 0.8 and right_mean < center_mean * 0.8
                                )
                                
                                is_portrait = edges_darker
                                if is_portrait:
                                    # Calculate the ratio of the center width to height
                                    center_width = third_width
                                    center_ratio = center_width / height
                                    ratio_tolerance = 0.1  # Default tolerance for max resolution images
                                    
                                    # Adjust tolerance based on the image URL
                                    if any(x in url for x in ['hqdefault', 'sddefault', 'mqdefault', 'default']):
                                        ratio_tolerance = 0.2
                                    
                                    # Check if the center ratio is within the acceptable tolerance
                                    is_portrait = abs(center_ratio - (9/16)) <= ratio_tolerance
                                
                                # Log detailed metrics used for portrait determination
                                _LOGGER.debug(f"Side means - Left: {left_mean:.1f}, Center: {center_mean:.1f}, Right: {right_mean:.1f}")
                                _LOGGER.debug(f"Dark percentiles - Left: {left_dark:.1f}, Right: {right_dark:.1f}")
                                _LOGGER.debug(f"Is portrait: {is_portrait}")
                                
                                return is_portrait
                                
                        except Exception as e:
                            # Log any exceptions that occur during the portrait check
                            _LOGGER.debug(f"Error in check_portrait for video {video_id}: {str(e)}")
                            return False
                    
                    thumbnails = video_data['snippet'].get('thumbnails', {})
                    poster_url = thumbnails.get('maxres', {}).get('url') or thumbnails.get('high', {}).get('url')
                    
                    # Log that the system is checking for portrait orientation in a short video
                    _LOGGER.critical(f"Checking portrait orientation for video {video_id} (<= 180s)")
                    
                    if poster_url and await check_portrait(poster_url):
                        # Log that the video is classified as a short with portrait orientation
                        _LOGGER.critical(f"Video {video_id} classified as short (portrait orientation)")
                        is_short = True
                    else:
                        # Log that the video is classified as a regular video
                        _LOGGER.critical(f"Video {video_id} classified as regular video (not portrait)")
                
                if is_short:
                    # Set the target list and maximum number of shorts videos
                    target_list = 'shorts_data'
                    max_videos = self.config_entry.options.get(CONF_MAX_SHORT_VIDEOS, DEFAULT_MAX_SHORT_VIDEOS)
                    _LOGGER.debug(f"Processing video {video_id} as short in {target_list} with max_videos={max_videos}")
                else:
                    # Set the target list and maximum number of regular videos
                    target_list = 'data'
                    max_videos = self.config_entry.options.get(CONF_MAX_REGULAR_VIDEOS, DEFAULT_MAX_REGULAR_VIDEOS)
                    video_data['runtime'] = str(max(1, int(duration_seconds / 60)))
                    _LOGGER.debug(f"Processing video {video_id} as regular video in {target_list} with max_videos={max_videos}")
                
                # Log the target list being used for processing the video
                _LOGGER.critical(f"Processing video {video_id} as {target_list}")
                
                if 'fanart' not in video_data:
                    # Log that fanart is being processed for the video
                    _LOGGER.debug(f"Processing fanart for video {video_id}")
                    video_data['fanart'] = await process_image_to_fanart(self.hass, video_id, thumbnails)
                
                if 'poster' not in video_data:
                    if poster_url:
                        # Prepare data for processing the portrait image
                        video_id_data = {'videoId': video_id, 'is_live': bool(is_live_stream and not actual_end)}
                        video_data['poster'] = await process_image_to_portrait(self.hass, poster_url, video_id_data)
                        _LOGGER.debug(f"Processed poster for video {video_id} from URL {poster_url}")
                    else:
                        # Set a default poster if no poster URL is available
                        video_data['poster'] = "/local/youtube_thumbnails/default_poster.jpg" if not is_live_stream else f"/local/youtube_thumbnails/{video_id}_live_portrait.jpg"
                        _LOGGER.debug(f"Set default poster for video {video_id}")
                
                if target_list not in self.data:
                    # Initialize the target list in data if it doesn't exist
                    self.data[target_list] = []
                    _LOGGER.debug(f"Initialized target list '{target_list}' in data")
                
                # Remove any existing entries of the video in both data and shorts_data
                self.data['data'] = [v for v in self.data.get('data', []) if v['id'] != video_data['id']]
                self.data['shorts_data'] = [v for v in self.data.get('shorts_data', []) if v['id'] != video_data['id']]
                
                # Set the airdate based on live stream status
                video_data['airdate'] = datetime.now(ZoneInfo("UTC")).isoformat() if is_live_stream else video_data.get('snippet', {}).get('publishedAt')
                self.data[target_list].insert(0, video_data)
                
                # Sort the videos by airdate in descending order and limit to max_videos
                self.data[target_list] = sorted(self.data[target_list], key=lambda x: x.get('airdate', ''), reverse=True)[:max_videos]
                _LOGGER.debug(f"Sorted and limited videos in {target_list} to {max_videos} entries")
                
                # Save the updated data asynchronously
                _LOGGER.debug(f"Saving updated data after processing video {video_id} to {target_list}")
                await self.store.async_save(self.data)
                
                if video_data and any(channel.lower() in video_data.get("snippet", {}).get("channelTitle", "").lower() 
                                    for channel in self.favorite_channels if channel):
                    # Log that the video is from a favorite channel and update favorite_channels list
                    _LOGGER.debug(f"Video {video_id} is from a favorite channel. Updating favorite_channels list.")
                    self.data["favorite_channels"] = [v for v in self.data.get("favorite_channels", []) if v["id"] != video_data["id"]]
                    self.data["favorite_channels"].insert(0, video_data)
                
                # Set the updated data
                self.async_set_updated_data(self.data)
                
                # Log that the video has been processed and saved to the target list
                _LOGGER.critical(f"Video {video_id} processed and saved to {target_list}")

    async def async_request_refresh(self, video_id=None, is_deleted=False):
        """Trigger a data refresh, possibly for a specific video."""
        if video_id:
            self.force_update = True
            self.webhook_video_id = video_id
            self.is_deleted = is_deleted
            await self.async_refresh()
        else:
            await super().async_request_refresh()
    
    def is_quota_exceeded(self):
        current_time = datetime.now(ZoneInfo("UTC"))
        _LOGGER.debug(f"Checking if quota is exceeded. Current quota: {self.youtube.current_quota}, Reset time: {self.youtube.quota_reset_time}")
        is_exceeded = self.youtube.current_quota >= 10000
        _LOGGER.debug(f"Quota exceeded: {is_exceeded}")
        return is_exceeded

    async def _validate_video_data(self, video):
        if isinstance(video, str):
            _LOGGER.debug(f"Received string instead of video data object: {video}")
            return False
            
        required_fields = ['id', 'snippet', 'contentDetails']
        if not all(field in video for field in required_fields):
            video_id = video.get('id', 'Unknown')
            _LOGGER.debug(f"Video ID {video_id} is missing required fields.")
            return False

        duration_iso = video['contentDetails'].get('duration', 'PT0M0S')
        try:
            duration_timedelta = isodate.parse_duration(duration_iso)
            duration_seconds = int(duration_timedelta.total_seconds())
            _LOGGER.debug(f"Video ID {video['id']} has duration_seconds: {duration_seconds}")
        except (ValueError, TypeError) as e:
            _LOGGER.debug(f"Video ID {video.get('id', 'Unknown')} has invalid duration format: {e}")
            return False

        # Exclude videos with duration <= 0
        if duration_seconds <= 0:
            _LOGGER.debug(f"Video ID {video['id']} excluded due to non-positive duration: {duration_seconds}")
            return False

        if video.get('live_status'):
            _LOGGER.debug(f"Video ID {video['id']} is a live stream.")
            video['runtime'] = ""
            return True
        elif duration_seconds <= 60 and not video.get('live_status'):
            _LOGGER.debug(f"Video ID {video['id']} classified as Short.")
            return True  # It's a Short; no need for 'statistics'
        elif duration_seconds <= 180:
            thumbnails = video['snippet'].get('thumbnails', {})
            thumb_url = thumbnails.get('maxres', {}).get('url') or thumbnails.get('high', {}).get('url')
            if not thumb_url:
                _LOGGER.debug(f"Video ID {video['id']} classified as Regular (no thumbnail).")
                return True
            async def check_portrait(url):
                try:
                    session = async_get_clientsession(self.hass)
                    async with session.get(url) as resp:
                        if resp.status != 200:
                            return False
                        image_data = await resp.read()
                    with Image.open(io.BytesIO(image_data)) as img:
                        width, height = img.size
                        target_ratio = 2 / 3
                        if width / height > target_ratio:
                            new_width = int(height * target_ratio)
                            potential_width = new_width
                            center_ratio = potential_width / height
                            if abs(center_ratio - (9/16)) <= 0.1 and new_width < width * 0.5:
                                return True
                        return False
                except Exception:
                    return False
            
            if await check_portrait(thumb_url):
                _LOGGER.debug(f"Video ID {video['id']} classified as Short (portrait thumbnail).")
                return True
            _LOGGER.debug(f"Video ID {video['id']} classified as Regular.")
            if 'statistics' not in video:
                video['statistics'] = {'viewCount': '0', 'likeCount': '0'}
            return True
        else:
            if 'statistics' not in video:
                video['statistics'] = {'viewCount': '0', 'likeCount': '0'}
            _LOGGER.debug(f"Video ID {video['id']} classified as Regular.")
            return True

    async def _async_update_data(self):
        stored_data = await self.store.async_load()
        if stored_data:
            self.data = {
                "data": stored_data.get("data", []),
                "shorts_data": stored_data.get("shorts_data", []),
                "favorite_channels": stored_data.get("favorite_channels", [])
            }
            self.favorite_channels = [channel.strip() for channel in self.config_entry.options.get(CONF_FAVORITE_CHANNELS, DEFAULT_FAVORITE_CHANNELS).split(',') if channel.strip()]
        else:
            self.data = {"data": [], "shorts_data": [], "favorite_channels": []}
            self.favorite_channels = [channel.strip() for channel in self.config_entry.options.get(CONF_FAVORITE_CHANNELS, DEFAULT_FAVORITE_CHANNELS).split(',') if channel.strip()]

        current_time = datetime.now(ZoneInfo('UTC'))
        is_initial_setup = not self.last_update_success

        if not is_initial_setup and self.last_webhook_time and (current_time - self.last_webhook_time) < timedelta(hours=6):
            _LOGGER.info("Skipping scheduled update due to recent webhook activity")
            await self.store.async_save(self.data)
            return self.data

        try:
            if self.youtube is None:
                _LOGGER.error("YouTube API instance is None in YouTubeDataUpdateCoordinator")
                await self.store.async_save(self.data)
                return {"data": [], "shorts_data": [], "info": "YouTube API instance is not initialized."}

            _LOGGER.debug("Fetching recent videos from YouTube API")
            videos = await self.youtube.get_recent_videos()

            if not videos['data'] and not videos['shorts_data']:
                message = "No new videos were fetched from YouTube API. Keeping existing data."
                _LOGGER.info(message)
                await self.store.async_save(self.data)
                return self.data

            _LOGGER.debug("Processing fetched videos")
            processed_data = await self._process_videos(videos['data'], "data")
            processed_shorts = await self._process_videos(videos['shorts_data'], "shorts_data")

            if processed_data.get('data') or processed_shorts.get('shorts_data'):
                _LOGGER.info(f"Found {len(processed_data.get('data', []))} new regular videos and {len(processed_shorts.get('shorts_data', []))} new shorts")
            else:
                _LOGGER.info("No new videos found in this update")

            existing_videos = {v['id']: v for v in self.data.get('data', [])}
            existing_shorts = {v['id']: v for v in self.data.get('shorts_data', [])}

            for video in processed_data.get('data', []):
                existing_videos[video['id']] = video
            
            for video in processed_shorts.get('shorts_data', []):
                existing_shorts[video['id']] = video

            merged_data = {
                "data": list(existing_videos.values()),
                "shorts_data": list(existing_shorts.values()),
                "favorite_channels": self.data.get("favorite_channels", [])
            }

            max_regular_videos = self.config_entry.options.get(CONF_MAX_REGULAR_VIDEOS, DEFAULT_MAX_REGULAR_VIDEOS)
            max_short_videos = self.config_entry.options.get(CONF_MAX_SHORT_VIDEOS, DEFAULT_MAX_SHORT_VIDEOS)
            
            all_regular_videos = sorted(merged_data['data'], key=lambda x: x.get('airdate', ''), reverse=True)
            all_shorts = sorted(merged_data['shorts_data'], key=lambda x: x.get('airdate', ''), reverse=True)
            merged_data['data'] = all_regular_videos[:max_regular_videos]
            merged_data['shorts_data'] = all_shorts[:max_short_videos]

            favorite_channels_data = self.data.get("favorite_channels", [])
            all_videos = all_regular_videos + all_shorts

            for video in all_videos:
                if (any(channel.lower() in video.get("snippet", {}).get("channelTitle", "").lower() 
                    for channel in self.favorite_channels if channel) and 
                    video['id'] not in {v['id'] for v in favorite_channels_data}):
                    favorite_channels_data.insert(0, video)

            merged_data['favorite_channels'] = favorite_channels_data

            self.data = merged_data
            await self.store.async_save(self.data)
            return self.data

        except QuotaExceededException as qee:
            _LOGGER.warning(f"API quota exceeded: {qee}")
            message = str(qee)
            persistent_notification.async_create(
                self.hass,
                message,
                title="YouTube API Quota Exceeded",
                notification_id="youtube_quota_exceeded"
            )
            await self.store.async_save(self.data)
            return self.data
        except Exception as err:
            _LOGGER.error(f"Error updating data from YouTube API: {err}", exc_info=True)
            await self.store.async_save(self.data)
            return self.data

    async def _process_videos(self, videos, video_type):
        if not videos:
            return {"data": [], "shorts_data": []}

        processed_videos = []
        shorts_videos = []

        for video in videos:
            if await self._validate_video_data(video):
                duration_iso = video['contentDetails'].get('duration', 'PT0M0S')
                try:
                    duration_timedelta = isodate.parse_duration(duration_iso)
                    duration_seconds = int(duration_timedelta.total_seconds())
                    runtime = max(1, int(duration_seconds / 60))
                    video['runtime'] = str(runtime)
                except (ValueError, TypeError):
                    _LOGGER.debug(f"Video ID {video.get('id', 'Unknown')} has invalid duration format.")
                    continue

                if get_USE_COMMENTS_AS_SUMMARY() and 'summary' not in video:
                    comments = await self.youtube.fetch_top_comments(video['id'])
                    video['summary'] = '\n\n'.join(comments) if comments else None
                elif not get_USE_COMMENTS_AS_SUMMARY():
                    video['summary'] = '\n\n'.join(video['snippet'].get('description', '').split('\n\n')[:2]).strip()

                # Process images
                video_id = video['id']
                thumbnails = video['snippet'].get('thumbnails', {})
                live_streaming_details = video.get('liveStreamingDetails', {})
                actual_start_time = live_streaming_details.get('actualStartTime')
                actual_end = live_streaming_details.get('actualEndTime')
                is_live_stream = bool(actual_start_time and not actual_end)

                try:
                    video_id_data = {'videoId': video_id, 'is_live': is_live_stream}
                    video['fanart'] = await process_image_to_fanart(self.hass, video_id_data, thumbnails)
                except Exception as e:
                    _LOGGER.error(f"Error processing fanart for video {video_id}: {e}")
                    video['fanart'] = "/local/youtube_thumbnails/default_fanart.jpg" if not is_live_stream else f"/local/youtube_thumbnails/{video_id}_live_fanart.jpg"

                # Prioritize 'maxres' thumbnail, fallback to 'high'
                if is_live_stream:
                    poster_url = f"https://i.ytimg.com/vi/{video_id}/maxresdefault_live.jpg"
                else:
                    poster_url = thumbnails.get('maxres', {}).get('url') or thumbnails.get('high', {}).get('url')

                if poster_url:
                    try:
                        video_id_data = {'videoId': video_id, 'is_live': is_live_stream}
                        video['poster'] = await process_image_to_portrait(self.hass, poster_url, video_id_data)
                        _LOGGER.debug(f"Using {'live' if is_live_stream else 'maxres' if thumbnails.get('maxres', {}).get('url') else 'high'} thumbnail for video {video_id}")
                    except Exception as e:
                        _LOGGER.error(f"Error processing poster for video {video_id}: {e}")
                        video['poster'] = "/local/youtube_thumbnails/default_poster.jpg" if not is_live_stream else f"/local/youtube_thumbnails/{video_id}_live_portrait.jpg"
                else:
                    _LOGGER.warning(f"No high-resolution thumbnail available for video: {video_id}")
                    video['poster'] = "/local/youtube_thumbnails/default_poster.jpg" if not is_live_stream else f"/local/youtube_thumbnails/{video_id}_live_portrait.jpg"

                if duration_seconds <= 60:
                    _LOGGER.debug(f"Video ID {video['id']} classified as Short.")
                    shorts_videos.append(video)
                elif duration_seconds <= 180:
                    async def check_portrait(url):
                        try:
                            session = async_get_clientsession(self.hass)
                            async with session.get(url) as resp:
                                if resp.status != 200:
                                    return False
                                image_data = await resp.read()
                            with Image.open(io.BytesIO(image_data)) as img:
                                width, height = img.size
                                target_ratio = 2 / 3
                                if width / height > target_ratio:
                                    new_width = int(height * target_ratio)
                                    if new_width < width * 0.5:
                                        return True
                                return False
                        except Exception:
                            return False

                    if poster_url and await check_portrait(poster_url):
                        _LOGGER.debug(f"Video ID {video['id']} classified as Short (portrait thumbnail).")
                        shorts_videos.append(video)
                    else:
                        _LOGGER.debug(f"Video ID {video['id']} classified as Regular video.")
                        processed_videos.append(video)
                else:
                    _LOGGER.debug(f"Video ID {video['id']} classified as Regular video.")
                    processed_videos.append(video)

        if video_type == "data":
            return {"data": processed_videos, "shorts_data": []}
        elif video_type == "shorts_data":
            return {"data": [], "shorts_data": shorts_videos}
        else:
            return {"data": processed_videos, "shorts_data": shorts_videos}

    async def delete_orphaned_images(self, processed_data, processed_shorts):
        save_dir = self.hass.config.path("www", "youtube_thumbnails")

        directory_exists = await self.hass.async_add_executor_job(os.path.exists, save_dir)
        if not directory_exists:
            _LOGGER.error(f"Directory does not exist: {save_dir}")
            return

        valid_files = set()

        # Include videos from processed data
        video_list = processed_data.get('data', []) + processed_shorts.get('shorts_data', [])

        # Include videos from persisted data
        persisted_data = await self.store.async_load()
        if persisted_data:
            video_list.extend(persisted_data.get('data', []))
            video_list.extend(persisted_data.get('shorts_data', []))
            video_list.extend(persisted_data.get('favorite_channels', []))

        for video in video_list:
            if isinstance(video, dict):
                video_id = video.get('id')
            elif isinstance(video, str):
                video_id = video
            else:
                _LOGGER.warning(f"Unexpected video format: {video}")
                continue

            if video_id:
                if isinstance(video, dict) and video.get('is_live'):
                    valid_files.add(f"{video_id}_live_portrait.jpg")
                    valid_files.add(f"{video_id}_live_fanart.jpg")
                else:
                    valid_files.add(f"{video_id}_portrait.jpg")
                    valid_files.add(f"{video_id}_fanart.jpg")

        # Add all previously fetched video IDs
        for video_id in self.youtube.fetched_video_ids:
            valid_files.add(f"{video_id}_portrait.jpg")
            valid_files.add(f"{video_id}_fanart.jpg")
            valid_files.add(f"{video_id}_live_portrait.jpg")
            valid_files.add(f"{video_id}_live_fanart.jpg")

        # _LOGGER.debug(f"Valid files to keep: {valid_files}")

        try:
            all_files = await aiofiles.os.listdir(save_dir)
            # _LOGGER.debug(f"All files in '{save_dir}': {all_files}")
        except Exception as e:
            _LOGGER.error(f"Error listing files in {save_dir}: {e}")
            return

        to_delete = [file_name for file_name in all_files if file_name not in valid_files and not file_name.startswith('default_')]

        _LOGGER.debug(f"Orphaned files to delete: {to_delete}")

        if not to_delete:
            _LOGGER.info("No orphaned files to delete.")
            return

        for file_name in to_delete:
            file_path = os.path.join(save_dir, file_name)
            _LOGGER.debug(f"Attempting to delete orphaned file: {file_path}")
            try:
                await aiofiles.os.remove(file_path)
                _LOGGER.info(f"Deleted orphaned file: {file_path}")
            except Exception as e:
                _LOGGER.error(f"Failed to delete orphaned file {file_path}: {e}")

class YouTubeRecentlyAddedSensor(CoordinatorEntity, SensorEntity):
    def __init__(self, coordinator):
        super().__init__(coordinator)
        self._attr_unique_id = f"youtube_recently_added"
        self._attr_name = f"YouTube Recently Added"
        self._attr_icon = "mdi:youtube"
        # _LOGGER.debug(f"Initializing YouTubeRecentlyAddedSensor with coordinator data: {coordinator.data}")

    async def async_added_to_hass(self):
        """Handle when entity is added to hass."""
        await super().async_added_to_hass()
        if self.coordinator.data:
            if isinstance(self.coordinator.data, dict) and "data" in self.coordinator.data:
                video_data = self.coordinator.data["data"]
                updated_video_data = []
                for video in video_data:
                    if isinstance(video, str):
                        video_details = await self.coordinator.youtube.get_video_by_id(video)
                        if video_details:
                            updated_video_data.append(video_details)
                    else:
                        updated_video_data.append(video)
                self.coordinator.data["data"] = updated_video_data
            _LOGGER.debug("Sensor added to hass with existing coordinator data.")
            self.async_write_ha_state()

    @property
    def state(self):
        if isinstance(self.coordinator.data, dict) and "data" in self.coordinator.data:
            video_data = self.coordinator.data["data"]
            _LOGGER.debug(f"Processing {len(video_data)} videos for state")
            
            processed_videos = []
            for video in video_data:
                if isinstance(video, dict) and "snippet" in video and "publishedAt" in video["snippet"]:
                    try:
                        processed_video = self._process_video_data(video)
                        if processed_video:
                            processed_videos.append(processed_video)
                    except Exception as e:
                        _LOGGER.error(f"Error processing video data: {e}", exc_info=True)

            sorted_videos = sorted(processed_videos, key=lambda x: x.get('airdate', ''), reverse=True)
            max_videos = self.coordinator.config_entry.options.get(CONF_MAX_REGULAR_VIDEOS, DEFAULT_MAX_REGULAR_VIDEOS)
            sorted_videos = sorted_videos[:max_videos]

            base_attributes = {
                "data": [
                    {
                        "title_default": "$title",
                        "line1_default": "$channel",
                        "line2_default": "$release",
                        "line3_default": "$runtime - $live_status - $genres",
                        "line4_default": "$views views - $likes likes",
                        "icon": "mdi:youtube"
                    }
                ],
                "friendly_name": "YouTube Recently Added",
                "current_quota": getattr(self.coordinator.youtube, 'current_quota', None)
            }

            self._prepared_items = []
            current_data = base_attributes["data"].copy()

            for item in sorted_videos:
                test_item = item.copy()
                test_attributes = base_attributes.copy()
                test_attributes["data"] = current_data + [test_item]
                if len(json.dumps(test_attributes).encode('utf-8')) > 16000:
                    break

                current_data.append(test_item)
                self._prepared_items.append(test_item)

            _LOGGER.debug(f"YouTubeRecentlyAddedSensor state (limited): {len(self._prepared_items)}")
            return len(self._prepared_items)
        _LOGGER.debug("YouTubeRecentlyAddedSensor state: 0 (no data available)")
        return 0

    @property
    def extra_state_attributes(self):
        attributes = {
            "data": [
                {
                    "title_default": "$title",
                    "line1_default": "$channel",
                    "line2_default": "$release",
                    "line3_default": "$runtime - $live_status - $genres",
                    "line4_default": "$views views - $likes likes",
                    "icon": "mdi:youtube"
                }
            ]
        }

        if hasattr(self, '_prepared_items'):
            attributes["data"].extend(self._prepared_items)
        else:
            if isinstance(self.coordinator.data, dict) and "data" in self.coordinator.data:
                video_data = self.coordinator.data["data"]
                video_data_list = []
                for video in video_data:
                    if isinstance(video, dict) and "snippet" in video and "publishedAt" in video["snippet"]:
                        try:
                            processed_video = self._process_video_data(video)
                            if processed_video:
                                if 'live_status' not in processed_video:
                                    processed_video['live_status'] = video.get('live_status', '')
                                video_data_list.append(processed_video)
                        except Exception as e:
                            _LOGGER.error(f"Error processing video data: {e}", exc_info=True)
                            continue

                sorted_video_data = sorted(video_data_list, key=lambda x: x.get('airdate', ''), reverse=True)
                
                current_data = attributes["data"]
                                
                for item in sorted_video_data[:self.state]:
                    test_item = item.copy()
                    test_attributes = attributes.copy()
                    test_attributes["data"] = current_data + [test_item]
                    if len(json.dumps(test_attributes).encode('utf-8')) > 16000:
                        _LOGGER.warning(f"YouTube Recently Added sensor items reduced to {len(current_data) - 1} due to 16KB attribute size limit.")
                        break

                    current_data.append(test_item)

        attributes["friendly_name"] = "YouTube Recently Added"
        if self.coordinator.last_update_success:
            attributes["current_quota"] = getattr(self.coordinator.youtube, 'current_quota', None)

        return attributes

    def _process_video_data(self, video):
        video_id = video.get('id')
        published_at = video["snippet"].get("publishedAt")
        if not published_at:
            _LOGGER.warning(f"Video {video_id} is missing 'publishedAt' field.")
            return None

        try:
            live_streaming_details = video.get('liveStreamingDetails', {})
            scheduled_start = live_streaming_details.get('scheduledStartTime')
            actual_start = live_streaming_details.get('actualStartTime')

            if scheduled_start and not actual_start:
                _LOGGER.debug(f"Skipping scheduled stream {video_id} that hasn't started yet")
                return None

            published_date = parser.isoparse(published_at).replace(tzinfo=ZoneInfo("UTC"))
            local_tz = ZoneInfo(self.hass.config.time_zone)
            local_published_date = published_date.astimezone(local_tz)
            airdate = local_published_date.isoformat()
            aired = local_published_date.strftime("%Y-%m-%d")
            release = local_published_date.strftime("%A, %B %d, %Y %I:%M %p")
            
            # Check if this is a live stream video and its status
            live_status = video.get('live_status', '')
            is_live = bool(live_status)
            stream_ended = live_status == "📴 Stream ended"
            
            # Pass live status and stream ended status to _convert_duration
            runtime = self._convert_duration(video['contentDetails'].get('duration', 'PT0M0S'), is_live, stream_ended)

            statistics = video.get('statistics', {})
            view_count = statistics.get('viewCount')
            like_count = statistics.get('likeCount')

            def format_count(count):
                if count is None:
                    return "0"
                count = int(count)
                if count >= 1000000:
                    value = count / 1000000
                    return f"{value:.1f}".rstrip('0').rstrip('.') + "M"
                elif count >= 1000:
                    value = count / 1000
                    return f"{value:.1f}".rstrip('0').rstrip('.') + "K"
                return str(count)

            views = format_count(view_count)
            likes = format_count(like_count)

            processed_data = {
                "id": video_id,
                "airdate": airdate,
                "aired": aired,
                "release": release,
                "title": video["snippet"].get("title", ""),
                "channel": video["snippet"].get("channelTitle", ""),
                "runtime": runtime,
                "genres": video.get("genres", ""),
                "views": views,
                "likes": likes,
                "summary": video.get("summary", video["snippet"].get("description", "")),
                "poster": video.get("poster", ""),
                "fanart": video.get("fanart", ""),
                "deep_link": f"https://www.youtube.com/watch?v={video_id}",
                "live_status": video.get("live_status", "")
            }
            
            # Add live_status if it exists
            if live_status:
                processed_data["live_status"] = live_status
                
            return processed_data
        except Exception as e:
            _LOGGER.error(f"Error processing video data for ID {video_id}: {e}", exc_info=True)
            return None

    def _convert_duration(self, duration, is_live=False, stream_ended=False):
        if is_live and not stream_ended:
            return ""
        try:
            duration_timedelta = isodate.parse_duration(duration)
            minutes = int(duration_timedelta.total_seconds() // 60)
            return str(max(1, minutes))
        except (isodate.ISO8601Error, ValueError, TypeError) as e:
            _LOGGER.error(f"Error converting duration: {e}")
            return ""

class YouTubeRecentlyAddedShortsSensor(YouTubeRecentlyAddedSensor):
    """Representation of a YouTube Recently Added Shorts Sensor."""

    def __init__(self, coordinator):
        super().__init__(coordinator)
        self._attr_unique_id = "youtube_recently_added_shorts"
        self._attr_name = "YouTube Recently Added Shorts"
        self._attr_icon = "mdi:youtube"
        _LOGGER.debug("Initializing YouTube Recently Added Shorts sensor")

    @property
    def state(self):
        _LOGGER.debug("Calculating state for YouTubeRecentlyAddedShortsSensor")
        if isinstance(self.coordinator.data, dict) and "shorts_data" in self.coordinator.data:
            shorts_data = self.coordinator.data["shorts_data"]
            _LOGGER.debug(f"Processing {len(shorts_data)} shorts for state")
            
            processed_shorts = []
            for video in shorts_data:
                if isinstance(video, dict) and "snippet" in video and "publishedAt" in video["snippet"]:
                    try:
                        processed_video = self._process_video_data(video)
                        if processed_video:
                            processed_shorts.append(processed_video)
                    except Exception as e:
                        _LOGGER.error(f"Error processing shorts data: {e}", exc_info=True)

            sorted_shorts = sorted(processed_shorts, key=lambda x: x.get('airdate', ''), reverse=True)
            max_videos = self.coordinator.config_entry.options.get(CONF_MAX_SHORT_VIDEOS, DEFAULT_MAX_SHORT_VIDEOS)
            sorted_shorts = sorted_shorts[:max_videos]

            base_attributes = {
                "data": [
                    {
                        "title_default": "$title",
                        "line1_default": "$channel",
                        "line2_default": "$release",
                        "line3_default": "$runtime - $live_status - $genres",
                        "line4_default": "$views views - $likes likes",
                        "icon": "mdi:youtube"
                    }
                ],
                "friendly_name": "YouTube Recently Added Shorts",
                "current_quota": getattr(self.coordinator.youtube, 'current_quota', None)
            }

            self._prepared_items = []
            current_data = base_attributes["data"].copy()

            for item in sorted_shorts:
                test_item = item.copy()
                test_attributes = base_attributes.copy()
                test_attributes["data"] = current_data + [test_item]
                if len(json.dumps(test_attributes).encode('utf-8')) > 16000:
                    break

                current_data.append(test_item)
                self._prepared_items.append(test_item)

            _LOGGER.debug(f"YouTubeRecentlyAddedShortsSensor state (limited): {len(self._prepared_items)}")
            return len(self._prepared_items)

        _LOGGER.warning("No shorts data available. Returning 0.")
        return 0

    @property
    def extra_state_attributes(self):
        attributes = {
            "data": [
                {
                    "title_default": "$title",
                    "line1_default": "$channel",
                    "line2_default": "$release",
                    "line3_default": "$runtime - $live_status - $genres",
                    "line4_default": "$views views - $likes likes",
                    "icon": "mdi:youtube"
                }
            ]
        }

        if hasattr(self, '_prepared_items'):
            attributes["data"].extend(self._prepared_items)
        else:
            if isinstance(self.coordinator.data, dict) and "shorts_data" in self.coordinator.data:
                shorts_data = self.coordinator.data["shorts_data"]
                shorts_data_list = []
                for video in shorts_data:
                    if isinstance(video, dict) and "snippet" in video and "publishedAt" in video["snippet"]:
                        try:
                            processed_video = self._process_video_data(video)
                            if processed_video:
                                shorts_data_list.append(processed_video)
                        except Exception as e:
                            _LOGGER.error(f"Error processing shorts data: {e}", exc_info=True)
                            continue

                sorted_shorts_data = sorted(shorts_data_list, key=lambda x: x.get('airdate', ''), reverse=True)
                
                current_data = attributes["data"]
                                
                for item in sorted_shorts_data[:self.state]:
                    test_item = item.copy()
                    test_attributes = attributes.copy()
                    test_attributes["data"] = current_data + [test_item]
                    if len(json.dumps(test_attributes).encode('utf-8')) > 16000:
                        _LOGGER.warning(f"YouTube Recently Added Shorts sensor items reduced to {len(current_data) - 1} due to 16KB attribute size limit.")
                        break

                    current_data.append(test_item)

        attributes["friendly_name"] = "YouTube Recently Added Shorts"
        if self.coordinator.last_update_success:
            attributes["current_quota"] = getattr(self.coordinator.youtube, 'current_quota', None)

        return attributes

class YouTubeRecentlyAddedFavoriteChannelsSensor(YouTubeRecentlyAddedSensor):
    def __init__(self, coordinator):
        super().__init__(coordinator)
        self._attr_unique_id = "youtube_recently_added_favorite_channels"
        self._attr_name = "YouTube Recently Added Favorite Channels"
        self._attr_icon = "mdi:youtube"

    @property
    def state(self):
        if isinstance(self.coordinator.data, dict) and "favorite_channels" in self.coordinator.data:
            favorite_videos = self.coordinator.data["favorite_channels"]
            
            base_attributes = {
                "data": [
                    {
                        "title_default": "$title",
                        "line1_default": "$channel",
                        "line2_default": "$release",
                        "line3_default": "$runtime - $live_status - $genres",
                        "line4_default": "$views views - $likes likes",
                        "icon": "mdi:youtube"
                    }
                ]
            }

            self._prepared_items = []
            current_data = base_attributes["data"].copy()

            sorted_videos = sorted(favorite_videos, key=lambda x: x.get('snippet', {}).get('publishedAt', ''), reverse=True)

            for item in sorted_videos:
                test_item = {
                    "id": item.get("id"),
                    "airdate": item.get("snippet", {}).get("publishedAt"),
                    "aired": item.get("snippet", {}).get("publishedAt", "").split("T")[0],
                    "release": datetime.strptime(item.get("snippet", {}).get("publishedAt"), "%Y-%m-%dT%H:%M:%SZ").strftime("%A, %B %d, %Y %I:%M %p"),
                    "title": item.get("snippet", {}).get("title"),
                    "channel": item.get("snippet", {}).get("channelTitle"),
                    "runtime": item.get("runtime", "1"),
                    "genres": item.get("genres", ""),
                    "views": self._process_video_data(item)['views'],
                    "likes": self._process_video_data(item)['likes'],
                    "summary": item.get("summary", item.get("snippet", {}).get("description", "")),
                    "poster": item.get("poster") or (
                        f"/local/youtube_thumbnails/{item.get('id')}_live_portrait.jpg" 
                        if item.get('liveStreamingDetails', {}).get('actualStartTime') and not item.get('liveStreamingDetails', {}).get('actualEndTime')
                        else f"/local/youtube_thumbnails/{item.get('id')}_portrait.jpg"
                    ),
                    "fanart": item.get("fanart") or (
                        f"/local/youtube_thumbnails/{item.get('id')}_live_fanart.jpg"
                        if item.get('liveStreamingDetails', {}).get('actualStartTime') and not item.get('liveStreamingDetails', {}).get('actualEndTime')
                        else f"/local/youtube_thumbnails/{item.get('id')}_fanart.jpg"
                    ),
                    "deep_link": f"https://www.youtube.com/watch?v={item.get('id')}"
                }

                test_attributes = base_attributes.copy()
                test_attributes["data"] = current_data + [test_item]
                if len(json.dumps(test_attributes).encode('utf-8')) > 16000:
                    break

                current_data.append(test_item)
                self._prepared_items.append(test_item)

            return len(self._prepared_items)
        return 0

    @property
    def extra_state_attributes(self):
        attributes = {
            "data": [
                {
                    "title_default": "$title",
                    "line1_default": "$channel",
                    "line2_default": "$release",
                    "line3_default": "$runtime - $live_status - $genres",
                    "line4_default": "$views views - $likes likes",
                    "icon": "mdi:youtube"
                }
            ]
        }

        if hasattr(self, '_prepared_items'):
            attributes["data"].extend(self._prepared_items)

        attributes["friendly_name"] = "YouTube Recently Added Favorite Channels"
        if self.coordinator.last_update_success:
            attributes["current_quota"] = getattr(self.coordinator.youtube, 'current_quota', None)

        return attributes
